
The following set of Alinity and Architect algorithms were implemented
using a combination of the statistical programming language R and AWS
Athena SQL.

This is the list of the currently available algorithms:

architect_dark_counts/            cc_cuvette_integrity/
architect_dark_counts_gt_avg250/  cc_cuvette_wash/
architect_dark_counts_gt_sd100/   ia_dark_counts/
architect_fe_pressure/            ia_fe_pressure/
architect_fe_pressure_i1/         ia_itv/
architect_fe_pressure_r1/         ia_pipettor_sample_syringe_backlash/
architect_fe_pressure_r2/         ia_process_jams_5756/
architect_fe_pressure_samp/       ia_process_jams_5758/
architect_fe_pressure_statsamp/   ia_vacuum_leak/
architect_wam_a/                  ia_vacuum_pump/
architect_wam_b/                  ia_vacuum_sensor/
architect_wam_c/                  ia_washzone_aspiration/

Each algorithm is composed of the following list of files:

common_utils.R
main.R
algorithm.R

The files main.R and algorithm.R are common to all of the algorithms
listed above. The file algorithm.R contains all the code and declarations
which are specific to the algorithm.

Common utils file contains the following functions which are used in all
the algorithms:

1) reliability_connect_to_db - main entry point for connecting to 
Reliability database.

2) reliability_dsn_connect_to_db - entry point to connect to Reliability
database using a DSN. Called from reliability_connect_to_db().

3) reliability_non_dsn_connect_to_db - entry point to connect to Reliability
database NOT using a DSN. Called from reliability_connect_to_db().

4) dx_connect_to_db - main entry point for connecting to DX database.

5) dx_custom_connect_to_db - entry point to connect to DX database when
a Customn connection is used.

6) dx_saml_connect_to_db - entry point to connect to DX database when a 
SAML connection is used.

7) spark_connect_to_db - main entry point for connecting to Spark to access
DX parquet files and to use Spark SQL via the R package Sparklyr.

8) connect_to_db - main entry point to connect to the DX data via Athena, 
IDA (retired) or Spark.

9) disconnect_from_db - disconnect from the DX database for both Athena, IDA
and Spark.

10) get_test_period - generate sampling period for running the algorithm. 
The number of days is passed in. Also has the option to get the dates from
a separate file for testing.

110 read_test_dates - read dates for sampling period from the file 
test_dates.csv. This function is used for testing and debugging.

12) read_csv_file - read in a CSV file and return as an R data frame.

13) query_subs = substitute values for variables in query templates.

14) make_save_to_file - pseudo-constructor for save_to_file() function. 
Save data between calls in the captured scope.

15) save_to_file - save a data frame to a file.

16) make_error_handler - pseudo-constructor for error handling object. 

17) errors - error handling object and functions.

18) make_write_results - pseudo-constructor for write_results() function. 
Save data between calls in the captured scope.

19) write_results - write flagged and healthy data to a CSV file.

20) empty_results - return an empty data frame.

21) exec_query - substitute values for query template variables and execute
query.

22) post_flagged_processing = default post flagged query processing which 
does nothing. It is overwritten by each algorithm when necessary.

23) spark_load_data - default function to load DX parquet files in to Spark
which does nothing. It is overwritten by Spark-based algorithms when
necessary.

The file main.R contains all the high-level logic which defines an
algorithm. The steps it executes are as follows:

    1) Set working directory.
    2) Load required R packages:
       - DBI
       - RJDBC
       - odbc
       - dplyr
       - sparklyr
    3) Source the common_utils.R file.
    4) Source the algorithm.R file.
    5) Start a try-catch block for error processing.
    6) Read in config.csv file which contains data for 
       connecting to external databases. This file is created
       by Apollo.
    7) Read in input.csv file which contains all the sets of 
       parameters required for the algorithm. This file is created
       by Apollo. Once the CSV file is read, then the data split
       into sets depending on the parameter PHM_PATTERNS_SK.
    8) Get start and end dates for algorithm.
    9) Open connection to DX database.
    10) Load parquet files for Spark (if needed).
    11) Connect to Reliability database if we have a reliability query.
    12) Loop over all parameters sets and repeat the following:
        1) Substitute parameters into flagged query template and execute
           the query.
        2) Substitute the parameters into healthy module SN query template
           and execute the query.
        3) If the Reliability query template is defined, then substitute 
           the parameters into the Reliability query template
           and execute the query.
        4) Process the query results. Call post_flagged_processing()
           function. The default version does nothing. Each algorithm
           will overwrite as needed.
        5) Using Reliability database data, suppress any module SN 
           which were recently visited.
        6) Remove flagged module SN from healthy module SN list.
        7) Write results to results.csv file.
    13) End of parameter sets loop. Close database connections.
    14) Exit with status equal to zero if no errors occurred, else 
    exit with a status of -1.

The algorithm-specific file algorithm.R contains the following:

1) flagged_query_template - template query defining the set of flagged
instruments.

2) modulesn_query_template - template query defining the potential set 
of healthy instruments.

3) reliability_query_template - template query determining the set
of instruments which were visited in the last 45 days.

4) number_of_days - number of days for the algorithm.

5) product_line_code - product line code for the instruments in the 
algorithm. Can be NA if the queries determine the product line code.

6) config_type - database type. Can be dx, ida, or spark.

7) post_flagged_processing - algorthm-specific function called
after the flagged query is execufed.

8) spark_load_data - algorithm-specific function for loading parquet files
into Spark memory.

