# Vector of words from all six books
words <- janeausten_words()

# Most frequent "a"-word that is at least 5 chars long
max_frequency(letter = 'a', words = words, min_length = 5)

# Partitioning
result <- lapply(letters, max_frequency,
                 words = words, min_length = 5) %>% unlist()

# barplot of result
barplot(result, las = 2)
----------------------------------------------------------------------
# Complete the function definition
mean_of_rnorm <- function(n) {
  # Generate normally distributed random numbers
  random_numbers <- rnorm(n)
  # Calculate the mean of the random numbers
  mean(random_numbers)
}

# Try it out
mean_of_rnorm(100)
----------------------------------------------------------------------
# From previous step
mean_of_rnorm <- function(n) {
  random_numbers <- rnorm(n)
  mean(random_numbers)
}

# Create a vector to store the results
result <- rep(NA, n_replicates)

# Set the random seed to 123
set.seed(123)

# Set up a for loop with iter from 1 to n_replicates
for (iter in 1:n_replicates) {
  # Call mean_of_rnorm with n_numbers_per_replicate
  result[iter] <- mean_of_rnorm(iter)
}

# View the result
hist(result)
----------------------------------------------------------------------
# From previous step
mean_of_rnorm <- function(n) {
  random_numbers <- rnorm(n)
  mean(random_numbers)
}

# Repeat n_numbers_per_replicate, n_replicates times
n <- rep(n_numbers_per_replicate, n_replicates)

# Call mean_of_rnorm() repeatedly using sapply()
result <- sapply(
  # The vectorized argument to pass
  n, 
  # The function to call
  mean_of_rnorm
)

# View the results
hist(result)
----------------------------------------------------------------------
# Function definition of ar1_multiple_blocks_of_trajectories()
ar1_multiple_blocks_of_trajectories <- function(ids, ...) {
  # Call ar1_block_of_trajectories() for each ids
  trajectories_by_block <- lapply(ids, ar1_block_of_trajectories, ...)
  
  # rbind results
  do.call(rbind, trajectories_by_block)
}

# Create a sequence from 1 to number of blocks
traj_ids <- 1:nrow(ar1est)

# Generate trajectories for all rows of the estimation dataset
trajs <- ar1_multiple_blocks_of_trajectories(
  ids = traj_ids, rate0 = 0.015,
  block_size = 10, traj_len = 15
)

# Show results
show_migration(trajs)
----------------------------------------------------------------------
# Load parallel
library(parallel)

# How many physical cores are available?
ncores <- detectCores(logical=FALSE)

# How many random numbers to generate
n <- ncores:1
----------------------------------------------------------------------
# From previous step
library(parallel)
ncores <- detectCores(logical = FALSE)
n <- ncores:1

# Use lapply to call rnorm for each n,
# setting mean to 10 and sd to 2 
lapply(n, rnorm, mean=10, sd=2)
----------------------------------------------------------------------
# From previous step
library(parallel)
ncores <- detectCores(logical = FALSE)
n <- ncores:1

# Create a cluster
cl <- makeCluster(ncores)

# Use clusterApply to call rnorm for each n in parallel,
# again setting mean to 10 and sd to 2 
clusterApply(cl, n, rnorm, mean=10, sd=2)

# Stop the cluster
stopCluster(cl)
----------------------------------------------------------------------
# Evaluate partial sums in parallel
part_sums <- clusterApply(cl, x = c(1,51),
                    fun = function(x) sum(x:(x + 49)))
# Total sum
total <- sum(unlist(part_sums))

# Check for correctness
total == sum(1:100)
----------------------------------------------------------------------
# Create a cluster and set parameters
cl <- makeCluster(2)
n_replicates <- 50
n_numbers_per_replicate <- 10000

# Parallel evaluation
means <- clusterApply(cl, 
             x = rep(n_numbers_per_replicate, n_replicates), 
             fun = mean_of_rnorm)
                
# View results as histogram
hist(unlist(means))
----------------------------------------------------------------------

In this exercise, you will explore the cluster object created by makeCluster(). In addition, you will use clusterCall() which evaluates a given function on all workers. This can be useful for example when retrieving information from workers.

clusterCall() takes two arguments: the cluster object and the function to apply to each worker node. Just like lapply(), the function is passed without parentheses.

Here, we will use it to determine the process ID of the workers, which is equivalent to finding process IDs of R sessions spawned by the master. Such info could be used for example for process management, including outside of R.

----------------------------------------------------------------------
# Load the parallel package
library(parallel)

# Make a cluster with 4 nodes
cl <- makeCluster(4)

# Investigate the structure of cl
str(cl)

# What is the process ID of the workers?
clusterCall(cl, Sys.getpid)

# Stop the cluster
stopCluster(cl)
----------------------------------------------------------------------

Socket vs. Fork (I)


Now we will explore differences between the socket and the fork backends. In a fork cluster, each worker is a copy of the master process, whereas socket workers start with an empty environment. We define a global object and check if workers have an access to it under each of the backends. Your job is to use the function clusterCall() to look for the global object in the workers' environment. The package parallel is available in your workspace.

----------------------------------------------------------------------
# A global variable and a print function are defined
a_global_var <- "before"
print_global_var <- function() print(a_global_var)

# Create a socket cluster with 2 nodes
cl_sock <- makeCluster(2, type="SOCK")

# Evaluate the print function on each node
clusterCall(cl_sock, print_global_var)

# Stop the cluster
stopCluster(cl_sock)
----------------------------------------------------------------------
# A global variable and a print function are defined
a_global_var <- "before"
print_global_var <- function() print(a_global_var)

# Create a fork cluster with 2 nodes
cl_fork <- makeCluster(2, type="FORK")

# Evaluate the print function on each node
clusterCall(cl_fork, print_global_var)

# Change the global var to "after"
a_global_var <- "after"

# Evaluate the print fun on each node again
clusterCall(cl_fork, print_global_var)

# Stop the cluster
stopCluster(cl_fork)
----------------------------------------------------------------------
# Wrap this code into a function
mean_of_rnorm_sequentially <- function(n_numbers_per_replicate,n_replicates) 
{
n <- rep(n_numbers_per_replicate, n_replicates)
lapply(n, mean_of_rnorm)
}

# Call it to try it
mean_of_rnorm_sequentially(1000, 5)
----------------------------------------------------------------------
# Wrap this code into a function
mean_of_rnorm_in_parallel <- function(n_numbers_per_replicate, n_replicates) {
n <- rep(n_numbers_per_replicate, n_replicates)
clusterApply(cl, n, mean_of_rnorm) 
}

# Call it to try it
mean_of_rnorm_in_parallel(1000, 5)
----------------------------------------------------------------------
# Set numbers per replicate to 5 million
n_numbers_per_replicate <- 5000000

# Set number of replicates to 4
n_replicates <- 4

# Run a microbenchmark
microbenchmark(
  # Call mean_of_rnorm_sequentially()
  mean_of_rnorm_sequentially(n_numbers_per_replicate, n_replicates), 
  # Call mean_of_rnorm_in_parallel()
  mean_of_rnorm_in_parallel(n_numbers_per_replicate, n_replicates),
  times = 1, 
  unit = "s"
)
----------------------------------------------------------------------
# Change the numbers per replicate to 100
n_numbers_per_replicate <- 100

# Change number of replicates to 100
n_replicates <- 100

# Rerun the microbenchmark
microbenchmark(
  mean_of_rnorm_sequentially(n_numbers_per_replicate, n_replicates), 
  mean_of_rnorm_in_parallel(n_numbers_per_replicate, n_replicates),
  times = 1, 
  unit = "s"
)
----------------------------------------------------------------------
# Pre-defined myrdnorm 
myrdnorm <- function(n, mean = 0, sd = 1) 
    rdnorm(n, mean = mean, sd = sd)

# Parameters
n_numbers_per_replicate <- 1000
n_replicates <- 20

# Repeat n_numbers_per_replicate, n_replicates times
n <- rep(n_numbers_per_replicate,n_replicates)

# Load extraDistr on master
library(extraDistr)

# Run myrdnorm in parallel. This should fail!
res <- clusterApply(cl, n, myrdnorm)
----------------------------------------------------------------------
# Pre-defined myrdnorm 
myrdnorm <- function(n, mean = 0, sd = 1) 
    rdnorm(n, mean = mean, sd = sd)

# Parameters
n_numbers_per_replicate <- 1000
n_replicates <- 20

# Repeat n_numbers_per_replicate, n_replicates times
n <- rep(n_numbers_per_replicate,n_replicates)

# Load extraDistr on master
library(extraDistr)

# load on all CPUs in cluster
clusterEvalQ(cl,library(extraDistr))

# Run myrdnorm in parallel. This should fail!
res <- clusterApply(cl, n, myrdnorm)

----------------------------------------------------------------------
# From previous step
myrdnorm <- function(n) {
  rdnorm(n, mean = mean, sd = sd)
}

# Set number of numbers to generate
n <- rep(1000, 20)

# Run an expression on each worker
clusterEvalQ(
  cl, {
    # Load extraDistr
    library(extraDistr)
    # Set mean to 10
    mean <- 10
    # Set sd to 5
    sd <- 5
})

# Run myrdnorm in parallel
res <- clusterApply(cl, n, myrdnorm)

# Plot the results
plot(table(unlist(res)))
----------------------------------------------------------------------

----------------------------------------------------------------------
----------------------------------------------------------------------
----------------------------------------------------------------------
----------------------------------------------------------------------
----------------------------------------------------------------------
----------------------------------------------------------------------
----------------------------------------------------------------------
----------------------------------------------------------------------
----------------------------------------------------------------------
----------------------------------------------------------------------
----------------------------------------------------------------------
